diff --git a/cache/week2/CORE-002/src/grid_batch_ops.c b/cache/week2/CORE-002/src/grid_batch_ops.c
index original..optimized 100644
--- a/cache/week2/CORE-002/src/grid_batch_ops.c
+++ b/cache/week2/CORE-002/src/grid_batch_ops.c
@@ -10,6 +10,32 @@
 #include <immintrin.h>  // For SIMD operations
 #include "../include/grid_callbacks.h"
 
+// ============================================================================
+// AVX-512 Optimizations
+// ============================================================================
+
+#ifdef __AVX512F__
+#define SIMD_WIDTH 16  // Process 16 cells at once with AVX-512
+#define USE_AVX512 1
+#else
+#define SIMD_WIDTH 8   // Fallback to AVX2
+#define USE_AVX512 0
+#endif
+
+// Performance hints
+#define likely(x)   __builtin_expect(!!(x), 1)
+#define unlikely(x) __builtin_expect(!!(x), 0)
+#define CACHE_LINE 64
+#define PREFETCH_DISTANCE 4
+
+// Aligned allocation for SIMD
+#define SIMD_ALIGN 64
+#define ALIGNED(x) __attribute__((aligned(x)))
+
+// Non-temporal store hints for large transfers
+#define STREAM_STORE_THRESHOLD (64 * 1024)  // 64KB
+
+// ============================================================================
+// Grid Cell Structure (optimized layout)
+// ============================================================================
+
 typedef struct {
     uint32_t character;  // Unicode character
     uint32_t fg_color;   // Foreground color (RGB)
@@ -17,6 +43,20 @@ typedef struct {
     uint16_t attributes; // Text attributes
 } grid_cell_t;
 
+// Optimized packed structure for SIMD operations
+typedef struct ALIGNED(SIMD_ALIGN) {
+    uint32_t characters[SIMD_WIDTH];
+    uint32_t fg_colors[SIMD_WIDTH];
+    uint32_t bg_colors[SIMD_WIDTH];
+    uint16_t attributes[SIMD_WIDTH];
+} grid_cell_block_t;
+
+// ============================================================================
+// AVX-512 Optimized Grid Clear
+// ============================================================================
+
+static inline void grid_clear_avx512(grid_cell_t* grid, size_t count) {
+#ifdef __AVX512F__
+    const __m512i zero = _mm512_setzero_si512();
+    const size_t blocks = count / 16;
+    const size_t remainder = count & 15;
+    
+    // Clear 16 cells at once (64 bytes)
+    for (size_t i = 0; i < blocks; i++) {
+        _mm512_store_si512((__m512i*)(grid + i * 16), zero);
+    }
+    
+    // Handle remainder
+    if (remainder > 0) {
+        __mmask16 mask = (1 << remainder) - 1;
+        _mm512_mask_store_epi32(grid + blocks * 16, mask, zero);
+    }
+#else
+    // AVX2 fallback
+    const __m256i zero = _mm256_setzero_si256();
+    const size_t blocks = count / 8;
+    
+    for (size_t i = 0; i < blocks; i++) {
+        _mm256_store_si256((__m256i*)(grid + i * 8), zero);
+        _mm256_store_si256((__m256i*)(grid + i * 8) + 1, zero);
+    }
+    
+    // Handle remainder with scalar
+    for (size_t i = blocks * 8; i < count; i++) {
+        memset(&grid[i], 0, sizeof(grid_cell_t));
+    }
+#endif
+}
+
+// ============================================================================
+// AVX-512 Optimized Grid Copy with Streaming Stores
+// ============================================================================
+
+static inline void grid_copy_streaming(grid_cell_t* dst, const grid_cell_t* src, size_t count) {
+    const size_t bytes = count * sizeof(grid_cell_t);
+    
+    // Use streaming stores for large transfers to bypass cache
+    if (bytes >= STREAM_STORE_THRESHOLD) {
+#ifdef __AVX512F__
+        const size_t blocks = count / 16;
+        
+        for (size_t i = 0; i < blocks; i++) {
+            __m512i data = _mm512_load_si512((__m512i*)(src + i * 16));
+            _mm512_stream_si512((__m512i*)(dst + i * 16), data);
+            
+            // Prefetch next cache lines
+            if (i + PREFETCH_DISTANCE < blocks) {
+                _mm_prefetch((const char*)(src + (i + PREFETCH_DISTANCE) * 16), _MM_HINT_T0);
+            }
+        }
+        
+        _mm_sfence();  // Ensure streaming stores complete
+#else
+        // AVX2 streaming stores
+        const size_t blocks = count / 8;
+        
+        for (size_t i = 0; i < blocks; i++) {
+            __m256i data1 = _mm256_load_si256((__m256i*)(src + i * 8));
+            __m256i data2 = _mm256_load_si256((__m256i*)(src + i * 8) + 1);
+            _mm256_stream_si256((__m256i*)(dst + i * 8), data1);
+            _mm256_stream_si256((__m256i*)(dst + i * 8) + 1, data2);
+        }
+        
+        _mm_sfence();
+#endif
+    } else {
+        // Regular copy for smaller transfers
+        memcpy(dst, src, bytes);
+    }
+}
+
+// ============================================================================
+// AVX-512 Pattern Fill Optimization
+// ============================================================================
+
+static inline void grid_fill_pattern_avx512(grid_cell_t* grid, size_t count, 
+                                             uint32_t character, uint32_t fg, uint32_t bg) {
+#ifdef __AVX512F__
+    // Create pattern vector
+    __m512i char_vec = _mm512_set1_epi32(character);
+    __m512i fg_vec = _mm512_set1_epi32(fg);
+    __m512i bg_vec = _mm512_set1_epi32(bg);
+    __m512i attr_vec = _mm512_setzero_si512();
+    
+    const size_t blocks = count / 16;
+    
+    // Unrolled loop for better performance
+    size_t i = 0;
+    for (; i + 4 <= blocks; i += 4) {
+        // Process 64 cells at once (4 x 16)
+        grid_cell_t* ptr = grid + i * 16;
+        
+        // Unrolled stores
+        _mm512_store_si512((__m512i*)&ptr[0].character, char_vec);
+        _mm512_store_si512((__m512i*)&ptr[16].character, char_vec);
+        _mm512_store_si512((__m512i*)&ptr[32].character, char_vec);
+        _mm512_store_si512((__m512i*)&ptr[48].character, char_vec);
+        
+        _mm512_store_si512((__m512i*)&ptr[0].fg_color, fg_vec);
+        _mm512_store_si512((__m512i*)&ptr[16].fg_color, fg_vec);
+        _mm512_store_si512((__m512i*)&ptr[32].fg_color, fg_vec);
+        _mm512_store_si512((__m512i*)&ptr[48].fg_color, fg_vec);
+        
+        _mm512_store_si512((__m512i*)&ptr[0].bg_color, bg_vec);
+        _mm512_store_si512((__m512i*)&ptr[16].bg_color, bg_vec);
+        _mm512_store_si512((__m512i*)&ptr[32].bg_color, bg_vec);
+        _mm512_store_si512((__m512i*)&ptr[48].bg_color, bg_vec);
+    }
+    
+    // Handle remaining blocks
+    for (; i < blocks; i++) {
+        grid_cell_t* ptr = grid + i * 16;
+        _mm512_store_si512((__m512i*)&ptr[0].character, char_vec);
+        _mm512_store_si512((__m512i*)&ptr[0].fg_color, fg_vec);
+        _mm512_store_si512((__m512i*)&ptr[0].bg_color, bg_vec);
+        _mm512_store_si512((__m512i*)&ptr[0].attributes, attr_vec);
+    }
+#else
+    // AVX2 fallback
+    __m256i char_vec = _mm256_set1_epi32(character);
+    __m256i fg_vec = _mm256_set1_epi32(fg);
+    __m256i bg_vec = _mm256_set1_epi32(bg);
+    
+    const size_t blocks = count / 8;
+    
+    for (size_t i = 0; i < blocks; i++) {
+        grid_cell_t* ptr = grid + i * 8;
+        _mm256_store_si256((__m256i*)&ptr[0].character, char_vec);
+        _mm256_store_si256((__m256i*)&ptr[0].fg_color, fg_vec);
+        _mm256_store_si256((__m256i*)&ptr[0].bg_color, bg_vec);
+    }
+#endif
+    
+    // Handle remainder
+    size_t processed = blocks * SIMD_WIDTH;
+    for (size_t i = processed; i < count; i++) {
+        grid[i].character = character;
+        grid[i].fg_color = fg;
+        grid[i].bg_color = bg;
+        grid[i].attributes = 0;
+    }
+}
+
+// ============================================================================
+// Optimized Dirty Region Tracking with Bit Manipulation
+// ============================================================================
+
+typedef struct ALIGNED(CACHE_LINE) {
+    uint64_t dirty_bits[16];  // 1024 bits for 1024 cells per row
+    int min_dirty_col;
+    int max_dirty_col;
+} dirty_row_t;
+
+static inline void mark_dirty_simd(dirty_row_t* row, int start_col, int count) {
+    const int end_col = start_col + count;
+    
+    // Use bit manipulation for efficient dirty tracking
+    for (int col = start_col; col < end_col; col += 64) {
+        int idx = col / 64;
+        int remaining = (end_col - col) < 64 ? (end_col - col) : 64;
+        uint64_t mask = (remaining == 64) ? ~0ULL : ((1ULL << remaining) - 1);
+        
+        __atomic_or_fetch(&row->dirty_bits[idx], mask << (col & 63), __ATOMIC_RELAXED);
+    }
+    
+    // Update bounds atomically
+    int current_min, current_max;
+    do {
+        current_min = __atomic_load_n(&row->min_dirty_col, __ATOMIC_RELAXED);
+    } while (start_col < current_min && 
+             !__atomic_compare_exchange_n(&row->min_dirty_col, &current_min, start_col,
+                                          false, __ATOMIC_RELAXED, __ATOMIC_RELAXED));
+    
+    do {
+        current_max = __atomic_load_n(&row->max_dirty_col, __ATOMIC_RELAXED);
+    } while (end_col > current_max && 
+             !__atomic_compare_exchange_n(&row->max_dirty_col, &current_max, end_col,
+                                          false, __ATOMIC_RELAXED, __ATOMIC_RELAXED));
+}
+
+// ============================================================================
+// Batch Operations with Prefetching
+// ============================================================================
+
 void grid_batch_update(grid_cell_t* grid, const grid_update_t* updates, size_t count) {
+    // Process updates in cache-friendly chunks
+    const size_t CHUNK_SIZE = 64;  // Fits in L1 cache
+    
+    for (size_t chunk = 0; chunk < count; chunk += CHUNK_SIZE) {
+        size_t chunk_end = (chunk + CHUNK_SIZE < count) ? chunk + CHUNK_SIZE : count;
+        
+        // Prefetch next chunk
+        if (chunk_end < count) {
+            for (size_t i = chunk_end; i < chunk_end + PREFETCH_DISTANCE && i < count; i++) {
+                __builtin_prefetch(&grid[updates[i].row * GRID_WIDTH + updates[i].col], 1, 1);
+            }
+        }
+        
+        // Process current chunk
+        for (size_t i = chunk; i < chunk_end; i++) {
+            const grid_update_t* update = &updates[i];
+            grid_cell_t* cell = &grid[update->row * GRID_WIDTH + update->col];
+            
+            cell->character = update->character;
+            cell->fg_color = update->fg_color;
+            cell->bg_color = update->bg_color;
+            cell->attributes = update->attributes;
+        }
+    }
+}
+
+// ============================================================================
+// Optimized Grid Comparison for Minimal Updates
+// ============================================================================
+
+static inline int grid_cells_equal_avx512(const grid_cell_t* a, const grid_cell_t* b, size_t count) {
+#ifdef __AVX512F__
+    const size_t blocks = count / 16;
+    
+    for (size_t i = 0; i < blocks; i++) {
+        __m512i va = _mm512_load_si512((__m512i*)(a + i * 16));
+        __m512i vb = _mm512_load_si512((__m512i*)(b + i * 16));
+        
+        __mmask16 mask = _mm512_cmpeq_epi32_mask(va, vb);
+        if (mask != 0xFFFF) {
+            return 0;  // Cells differ
+        }
+    }
+    
+    // Check remainder
+    for (size_t i = blocks * 16; i < count; i++) {
+        if (memcmp(&a[i], &b[i], sizeof(grid_cell_t)) != 0) {
+            return 0;
+        }
+    }
+    
+    return 1;  // All cells equal
+#else
+    return memcmp(a, b, count * sizeof(grid_cell_t)) == 0;
+#endif
+}
+
+// ============================================================================
+// Public API with Optimizations
+// ============================================================================
+
+void grid_clear_optimized(grid_cell_t* grid, size_t width, size_t height) {
+    grid_clear_avx512(grid, width * height);
+}
+
+void grid_copy_optimized(grid_cell_t* dst, const grid_cell_t* src, size_t width, size_t height) {
+    grid_copy_streaming(dst, src, width * height);
+}
+
+void grid_fill_optimized(grid_cell_t* grid, size_t width, size_t height,
+                         uint32_t character, uint32_t fg, uint32_t bg) {
+    grid_fill_pattern_avx512(grid, width * height, character, fg, bg);
+}