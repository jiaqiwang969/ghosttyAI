# P0 Critical Defect Fixes
# Generated by QA-002 for Task T-404
# Date: 2025-08-26

--- a/cache/week2/CORE-001/src/event_loop_router.c
+++ b/cache/week2/CORE-001/src/event_loop_router.c
@@ -40,9 +40,16 @@ static void libevent_cleanup(void* base) {
 static int libevent_event_add(void* base, event_handle_t* handle, const struct timeval* timeout) {
+    if (!handle) {
+        errno = EINVAL;
+        return -1;
+    }
+    
     struct event* ev = (struct event*)handle->backend_data;
     if (!ev) {
         ev = event_new((struct event_base*)base, handle->fd, handle->events,
                       handle->callback, handle->user_data);
+        if (!ev) {
+            return -1;  // Memory allocation failed
+        }
         handle->backend_data = ev;
     }
     return event_add(ev, timeout);
@@ -50,6 +57,11 @@ static int libevent_event_add(void* base, event_handle_t* handle, const struct 
 
 static int libevent_event_del(void* base, event_handle_t* handle) {
+    if (!handle) {
+        errno = EINVAL;
+        return -1;
+    }
+    
     struct event* ev = (struct event*)handle->backend_data;
     if (!ev) return -1;
-    return event_del(ev);
+    int ret = event_del(ev);
+    if (ret == 0) {
+        event_free(ev);
+        handle->backend_data = NULL;
+    }
+    return ret;
 }

--- a/cache/week2/CORE-002/src/grid_batch_ops.c
+++ b/cache/week2/CORE-002/src/grid_batch_ops.c
@@ -128,8 +128,18 @@ static inline void grid_memcpy_prefetch(grid_cell_internal_t* dst,
             __builtin_prefetch(&src[i + PREFETCH_DISTANCE], 0, 3);
         }
         
-        __m256i data1 = _mm256_load_si256((__m256i*)(src + i));
-        __m256i data2 = _mm256_load_si256((__m256i*)(src + i + 2));
+        // Check alignment and use appropriate load instruction
+        __m256i data1, data2;
+        if (((uintptr_t)(src + i) & 31) == 0) {
+            // Aligned load
+            data1 = _mm256_load_si256((__m256i*)(src + i));
+            data2 = _mm256_load_si256((__m256i*)(src + i + 2));
+        } else {
+            // Unaligned load (safer but slightly slower)
+            data1 = _mm256_loadu_si256((__m256i*)(src + i));
+            data2 = _mm256_loadu_si256((__m256i*)(src + i + 2));
+        }
+        
         _mm256_stream_si256((__m256i*)(dst + i), data1);
         _mm256_stream_si256((__m256i*)(dst + i + 2), data2);
     }
@@ -147,10 +157,17 @@ static inline void grid_memcpy_prefetch(grid_cell_internal_t* dst,
 
 static void* grid_backend_init(uint32_t width, uint32_t height, uint32_t history_limit) {
+    if (width == 0 || height == 0 || width > 10000 || height > 10000) {
+        errno = EINVAL;
+        return NULL;
+    }
+    
     grid_backend_impl_t* backend = calloc(1, sizeof(grid_backend_impl_t));
     if (!backend) return NULL;
+    
+    // Initialize mutex for thread safety
+    pthread_mutex_init(&backend->mutex, NULL);
+    pthread_rwlock_init(&backend->rwlock, NULL);

@@ -175,6 +192,42 @@ static void* grid_backend_init(uint32_t width, uint32_t height, uint32_t histor
     return backend;
 }

+// Thread-safe wrapper for grid operations
+static int grid_set_cell_safe(void* backend_ptr, uint32_t x, uint32_t y, 
+                              const grid_cell_t* cell) {
+    grid_backend_impl_t* backend = (grid_backend_impl_t*)backend_ptr;
+    if (!backend || !cell) {
+        return -1;
+    }
+    
+    // Use reader-writer lock for better performance
+    pthread_rwlock_wrlock(&backend->rwlock);
+    
+    // Bounds checking
+    if (x >= backend->width || y >= backend->height) {
+        pthread_rwlock_unlock(&backend->rwlock);
+        return -1;
+    }
+    
+    // Perform the operation
+    grid_cell_internal_t* target = &backend->lines[y][x];
+    target->utf8_data = cell->data;
+    target->attr = cell->attr;
+    target->fg = cell->fg;
+    target->bg = cell->bg;
+    target->flags = cell->flags;
+    
+    // Update dirty tracking
+    if (backend->dirty_tracking) {
+        if (x < backend->dirty.min_x) backend->dirty.min_x = x;
+        if (x > backend->dirty.max_x) backend->dirty.max_x = x;
+        if (y < backend->dirty.min_y) backend->dirty.min_y = y;
+        if (y > backend->dirty.max_y) backend->dirty.max_y = y;
+    }
+    
+    pthread_rwlock_unlock(&backend->rwlock);
+    return 0;
+}
+
 // ============================================================================
 // Batch Operations with Optimization
 // ============================================================================
@@ -183,6 +236,16 @@ static int grid_backend_begin_batch(void* backend_ptr) {
     grid_backend_impl_t* backend = (grid_backend_impl_t*)backend_ptr;
     if (!backend) return -1;
     
+    pthread_mutex_lock(&backend->mutex);
+    
+    if (backend->batch.active) {
+        // Already in batch mode
+        pthread_mutex_unlock(&backend->mutex);
+        return -1;
+    }
+    
     backend->batch.active = true;
     backend->batch.start_time = get_time_ns();
+    
+    pthread_mutex_unlock(&backend->mutex);
     return 0;
 }